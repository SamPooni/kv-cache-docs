<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 7: Intelligent KV-Cache Management ‚Äî KV-Cache Offloading Architecture</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@300;400;500;600;700&family=IBM+Plex+Mono:wght@400;500;600&family=Fraunces:opsz,wght@9..144,400;9..144,600;9..144,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .figure-embed {
      background: var(--bg-card);
      border: 1px solid var(--border-primary);
      border-radius: 12px;
      margin: 2rem 0;
      overflow: hidden;
    }
    .figure-embed-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 1rem 1.5rem;
      background: var(--bg-tertiary);
      border-bottom: 1px solid var(--border-primary);
    }
    .figure-embed-label {
      font-family: var(--font-mono);
      font-size: 0.9rem;
      color: var(--accent-cyan);
    }
    .figure-embed-link {
      font-size: 0.85rem;
      color: var(--accent-purple);
      text-decoration: none;
    }
    .figure-embed-link:hover { text-decoration: underline; }
    .figure-embed iframe {
      width: 100%;
      height: 500px;
      border: none;
      background: #fff;
    }
    .figure-embed.tall iframe { height: 700px; }
    .figure-embed.short iframe { height: 400px; }
  </style>
</head>
<body>
  <div class="watermark">¬© 2025 Subramaniyam (Sam) Pooni<br>All Rights Reserved<br>Proprietary & Confidential</div>

  <nav class="nav">
    <div class="nav-inner">
      <a href="../index.html" class="nav-brand"><span class="nav-brand-icon">‚ö°</span> KV-Cache Architecture</a>
      <div class="nav-links">
        <a href="ch00-executive-summary.html" class="nav-link"><span class="nav-chapter-num">0</span> Summary</a>
        <a href="ch03-architecture.html" class="nav-link"><span class="nav-chapter-num">3</span> Architecture</a>
        <a href="ch07-kv-cache.html" class="nav-link active"><span class="nav-chapter-num">7</span> KV-Cache</a>
        <a href="../figures/index.html" class="nav-link">üìä Figures</a>
        <a href="../appendix/index.html" class="nav-link">Appendix</a>
      </div>
    </div>
  </nav>

  <div class="page-wrapper">
    <div class="container">
      <header class="chapter-header">
        <div class="chapter-label">Chapter 7</div>
        <h1 class="chapter-title">Intelligent KV-Cache Management</h1>
        <p class="chapter-subtitle">Per-head tracking, EMA-based scoring, and RoPE-aware prefetching ‚Äî the core algorithmic innovations that achieve 95% HBM hit rate.</p>
      </header>

      <div class="stats-grid" style="margin-bottom: 3rem;">
        <div class="stat-box"><div class="stat-value">27</div><div class="stat-label">Figures in Chapter</div></div>
        <div class="stat-box"><div class="stat-value">95%</div><div class="stat-label">HBM Hit Rate</div></div>
        <div class="stat-box"><div class="stat-value">+25%</div><div class="stat-label">vs LRU Baseline</div></div>
        <div class="stat-box"><div class="stat-value">7.5%</div><div class="stat-label">Latency Overhead</div></div>
      </div>

      <!-- Section 7.1: The Memory Wall -->
      <h2>7.1 The Memory Wall Problem</h2>
      
      <p>KV-cache grows linearly with context length. At 128K tokens, Llama-70B requires <strong>41 GB per user</strong>‚Äîexceeding single-GPU capacity for multi-user scenarios.</p>

      <div class="figure-embed">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.1 ‚Äî Memory Wall Visualization</span>
          <a href="../figures/ch07-kv-cache/diagram-memory-wall-html.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/diagram-memory-wall-html.html"></iframe>
      </div>

      <!-- Section 7.2: Head Specialization -->
      <h2>7.2 Attention Head Specialization</h2>
      
      <p>Research reveals that attention heads specialize into distinct functional roles. Treating them uniformly wastes optimization potential.</p>

      <div class="table-wrapper">
        <table>
          <thead>
            <tr><th>Head Type</th><th>% of Heads</th><th>Attention Pattern</th><th>Cache Strategy</th></tr>
          </thead>
          <tbody>
            <tr><td style="color: var(--accent-cyan);"><strong>Recency</strong></td><td>~40%</td><td>Last 50-200 tokens</td><td>Keep recent context hot</td></tr>
            <tr><td style="color: var(--accent-green);"><strong>Anchor</strong></td><td>~15%</td><td>Positions 0-100 (system prompt)</td><td>Pin permanently</td></tr>
            <tr><td style="color: var(--accent-purple);"><strong>Retrieval</strong></td><td>~25%</td><td>Content-based lookup</td><td>Use EMA scoring</td></tr>
            <tr><td style="color: var(--accent-orange);"><strong>Syntactic</strong></td><td>~20%</td><td>Grammar patterns</td><td>Sparse, pattern-based</td></tr>
          </tbody>
        </table>
      </div>

      <div class="figure-embed">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.2 ‚Äî Attention Locality Patterns</span>
          <a href="../figures/ch07-kv-cache/figure-3-3-attention-locality.html" target="_blank" class="figure-embed-link">View TSX Source ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/figure-3-3-attention-locality.html" style="background: #1a1a2e;"></iframe>
      </div>

      <!-- Section 7.3: GQA Structure -->
      <h2>7.3 Grouped Query Attention (GQA)</h2>
      
      <p>Modern models like Llama use GQA where multiple query heads share KV heads. Llama-70B has 64 query heads sharing 8 KV heads‚Äîan 8√ó reduction in KV-cache size.</p>

      <div class="figure-embed">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.3 ‚Äî GQA Structure Explained</span>
          <a href="../figures/ch07-kv-cache/gqa-explainer.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/gqa-explainer.html"></iframe>
      </div>

      <div class="figure-embed">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.4 ‚Äî GQA Tracking Diagram</span>
          <a href="../figures/ch07-kv-cache/gqa-tracking-diagram.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/gqa-tracking-diagram.html"></iframe>
      </div>

      <!-- Section 7.4: Per-Head Tracking -->
      <h2>7.4 Per-Head Importance Tracking</h2>
      
      <p>A token might be cold for recency heads (position 5000) but hot for retrieval heads (contains key information). Token-level eviction would incorrectly evict this token. <strong>Per-head tracking preserves it.</strong></p>

      <div class="equation">
        P<sub>aggregate</sub>(position) = max<sub>h‚ààheads</sub>(P<sub>h</sub>(position))
        <span class="equation-label">Position survives if ANY head needs it</span>
      </div>

      <div class="figure-embed tall">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.5 ‚Äî Per-Head Tracking Visualization</span>
          <a href="../figures/ch07-kv-cache/diagram-per-head-html.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/diagram-per-head-html.html"></iframe>
      </div>

      <div class="figure-embed">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.6 ‚Äî Per-Head Score Matrix</span>
          <a href="../figures/ch07-kv-cache/per-head-tracking.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/per-head-tracking.html"></iframe>
      </div>

      <!-- Section 7.5: EMA Scoring -->
      <h2>7.5 EMA-Based Attention Scoring</h2>
      
      <p>Simple LRU fails because important tokens (system prompts) may not be recently accessed but are critical. We use Exponential Moving Average to capture sustained importance:</p>

      <div class="equation">
        score<sub>t</sub>(p) = Œ± √ó attention<sub>t</sub>(p) + (1 - Œ±) √ó score<sub>t-1</sub>(p)
        <span class="equation-label">Œ± = 0.1 recommended (half-life ‚âà 7 decode steps)</span>
      </div>

      <div class="callout insight">
        <div class="callout-title">üí° Why EMA Beats LRU</div>
        <p style="margin-bottom: 0;"><strong>System prompt at position 5</strong> receiving 4% attention every step: LRU evicts after ~100 steps (hasn't been "accessed recently"). EMA maintains stable score of 0.04, <strong>never evicted</strong>.</p>
      </div>

      <div class="figure-embed tall">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.7 ‚Äî EMA Scoring Algorithm</span>
          <a href="../figures/ch07-kv-cache/diagram-ema-scoring-html.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/diagram-ema-scoring-html.html"></iframe>
      </div>

      <div class="figure-embed">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.8 ‚Äî EMA Step-by-Step Calculation</span>
          <a href="../figures/ch07-kv-cache/ema-calculation.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/ema-calculation.html"></iframe>
      </div>

      <div class="figure-embed">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.9 ‚Äî EMA Eviction Policy</span>
          <a href="../figures/ch07-kv-cache/ema-eviction.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/ema-eviction.html"></iframe>
      </div>

      <!-- Section 7.6: RoPE-Aware Prefetch -->
      <h2>7.6 RoPE-Aware Prefetching</h2>
      
      <p>Rotary Position Embeddings (RoPE) create distance-dependent attention decay. Attention naturally concentrates on nearby positions:</p>

      <div class="equation">
        Attention(q<sub>m</sub>, k<sub>n</sub>) ‚àù cos((m-n)Œ∏)
        <span class="equation-label">Attention decays with position distance |m-n|</span>
      </div>

      <div class="figure-embed">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.10 ‚Äî RoPE Distance Decay</span>
          <a href="../figures/ch07-kv-cache/rope-prefetch.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/rope-prefetch.html"></iframe>
      </div>

      <div class="figure-embed">
        <div class="figure-embed-header">
          <span class="figure-embed-label">Figure 7.11 ‚Äî RoPE Prefetch Example</span>
          <a href="../figures/ch07-kv-cache/rope-prefetch-example.html" target="_blank" class="figure-embed-link">Open Full Screen ‚Üó</a>
        </div>
        <iframe src="../figures/ch07-kv-cache/rope-prefetch-example.html"></iframe>
      </div>

      <!-- Section 7.7: Combined Results -->
      <h2>7.7 Combined Hit Rate Results</h2>
      
      <p>Each algorithmic improvement contributes to the final 95% HBM hit rate:</p>

      <div class="bar-chart">
        <div class="bar-row">
          <span class="bar-label">LRU baseline</span>
          <div class="bar-container">
            <div class="bar-fill" style="width: 70%; background: var(--accent-red);">70%</div>
          </div>
        </div>
        <div class="bar-row">
          <span class="bar-label">+ Anchor pinning</span>
          <div class="bar-container">
            <div class="bar-fill" style="width: 78%; background: var(--accent-orange);">78% (+8%)</div>
          </div>
        </div>
        <div class="bar-row">
          <span class="bar-label">+ EMA scoring</span>
          <div class="bar-container">
            <div class="bar-fill" style="width: 85%; background: var(--accent-yellow);">85% (+7%)</div>
          </div>
        </div>
        <div class="bar-row">
          <span class="bar-label">+ Per-head tracking</span>
          <div class="bar-container">
            <div class="bar-fill" style="width: 91%; background: var(--accent-cyan);">91% (+6%)</div>
          </div>
        </div>
        <div class="bar-row">
          <span class="bar-label">+ RoPE prefetch</span>
          <div class="bar-container">
            <div class="bar-fill" style="width: 95%; background: var(--accent-green);">95% (+4%)</div>
          </div>
        </div>
      </div>

      <div class="stats-grid">
        <div class="stat-box"><div class="stat-value">95%</div><div class="stat-label">Final Hit Rate</div></div>
        <div class="stat-box"><div class="stat-value">+25%</div><div class="stat-label">vs LRU</div></div>
        <div class="stat-box"><div class="stat-value">7.5%</div><div class="stat-label">Latency Overhead</div></div>
      </div>

      <!-- Chapter Navigation -->
      <div class="chapter-nav">
        <a href="ch06-preprocessing.html" class="chapter-nav-btn prev">
          <span class="chapter-nav-label">‚Üê Previous</span>
          <span class="chapter-nav-title">Chapter 6: Preprocessing</span>
        </a>
        <a href="ch08-moe.html" class="chapter-nav-btn next">
          <span class="chapter-nav-label">Next ‚Üí</span>
          <span class="chapter-nav-title">Chapter 8: MoE Routing</span>
        </a>
      </div>
    </div>

    <footer class="footer">
      <div style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid var(--border-primary);">
        <p style="font-size: 0.8rem; color: var(--text-muted);">
          ¬© 2025 Subramaniyam (Sam) Pooni. All Rights Reserved.<br>
          This document contains proprietary and confidential information.
        </p>
      </div>
      <p>KV-Cache Offloading Architecture ‚Äî v3.0</p>
    </footer>
  </div>
</body>
</html>
