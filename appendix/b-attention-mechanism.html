<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Appendix B: Attention Mechanism — KV-Cache Offloading</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@300;400;500;600;700&family=IBM+Plex+Mono:wght@400;500;600&family=Fraunces:opsz,wght@9..144,400;9..144,600;9..144,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
</head>
<body>
  <div class="watermark">
    © 2025 Subramaniyam (Sam) Pooni<br>
    All Rights Reserved<br>
    Proprietary & Confidential
  </div>

  <nav class="nav"><div class="nav-inner"><a href="../index.html" class="nav-brand"><span class="nav-brand-icon">⚡</span> KV-Cache Architecture</a></div></nav>
  <div class="page-wrapper">
    <div class="container">
      <header class="chapter-header">
        <div class="chapter-label">Appendix B</div>
        <h1 class="chapter-title">Attention Mechanism Deep Dive</h1>
        <p class="chapter-subtitle">Scaled dot-product attention, multi-head structure, and grouped-query attention (GQA).</p>
      </header>

      <h2>B.1 Scaled Dot-Product Attention</h2>
      <div class="equation">Attention(Q, K, V) = softmax(QK<sup>T</sup> / √d<sub>k</sub>) × V</div>
      <p>Steps:</p>
      <ol>
        <li><strong>Score:</strong> QK<sup>T</sup> computes similarity between queries and keys</li>
        <li><strong>Scale:</strong> Divide by √d<sub>k</sub> to prevent gradient vanishing</li>
        <li><strong>Normalize:</strong> Softmax converts scores to probabilities</li>
        <li><strong>Aggregate:</strong> Weighted sum of values</li>
      </ol>

      <h2>B.2 Multi-Head Attention</h2>
      <p>Instead of single attention, we use multiple parallel attention heads:</p>
      <div class="equation">MultiHead(Q, K, V) = Concat(head<sub>1</sub>, ..., head<sub>h</sub>)W<sup>O</sup></div>
      <p>For Llama-70B: 64 query heads, each with d<sub>head</sub>=128.</p>

      <h2>B.3 Grouped-Query Attention (GQA)</h2>
      <p>GQA reduces KV-cache by sharing K/V heads across multiple query heads:</p>
      <div class="table-wrapper">
        <table>
          <thead><tr><th>Type</th><th>Q Heads</th><th>KV Heads</th><th>KV per token</th></tr></thead>
          <tbody>
            <tr><td>MHA</td><td class="mono">64</td><td class="mono">64</td><td class="mono danger">16 KB/layer</td></tr>
            <tr><td>GQA (Llama-70B)</td><td class="mono">64</td><td class="mono">8</td><td class="mono highlight">2 KB/layer</td></tr>
            <tr><td>MQA</td><td class="mono">64</td><td class="mono">1</td><td class="mono">256 B/layer</td></tr>
          </tbody>
        </table>
      </div>
      <p><strong>GQA achieves 8× KV-cache reduction</strong> with minimal quality loss.</p>

      <div class="chapter-nav">
        <a href="a-transformer-fundamentals.html" class="chapter-nav-btn prev"><span class="chapter-nav-label">← Previous</span><span class="chapter-nav-title">Appendix A</span></a>
        <a href="c-kv-cache-math.html" class="chapter-nav-btn next"><span class="chapter-nav-label">Next →</span><span class="chapter-nav-title">Appendix C</span></a>
      </div>
    </div>
    <footer class="footer">
      <div style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid var(--border-primary);">
        <p style="font-size: 0.8rem; color: var(--text-muted);">
          © 2025 Subramaniyam (Sam) Pooni. All Rights Reserved.<br>
          This document contains proprietary and confidential information.<br>
          Unauthorized reproduction or distribution is strictly prohibited.
        </p>
      </div>
      <p>Distributed Endpoint Architecture for KV-Cache Offloading in LLM Inference</p>
      <p style="margin-top: 0.5rem;">Technical Documentation v3.0 — December 2025</p>
    </footer>
  </div>
</body>
</html>
