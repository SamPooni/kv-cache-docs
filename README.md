# kv-cache-docs
Distributed Endpoint Architecture for KV-Cache Offloading in LLM Inference. Achieves 6× memory expansion, 8× user capacity, 95% hit rate, and 36% cost reduction using CXL 3.0 computational storage devices with intelligent cache management.
